{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1308"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from cassis import * \n",
    "path = \"./curation\"\n",
    "#example path = \"./curation/elections/file.json\"\n",
    "files = []\n",
    "file_names= []\n",
    "for sub_folder in os.listdir(path):\n",
    "    if sub_folder.startswith('input'):\n",
    "        for file in os.listdir(os.path.join(path, sub_folder)):\n",
    "            if file.endswith('.json'):\n",
    "                with open(os.path.join(path, sub_folder,file), 'rb') as f:\n",
    "                    cas = load_cas_from_json(f)\n",
    "                    files.append(cas)\n",
    "                file_names.append(sub_folder)\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_part026.txt', 'input_part950.txt', 'input_part368.txt', 'input_part_1161.txt', 'input_part011.txt', 'input_part029.txt', 'input_part541.txt', 'input_part297.txt', 'input_part670.txt', 'input_part956.txt', 'input_part_1118.txt', 'input_part482.txt', 'input_part946.txt', 'input_part503.txt', 'input_part449.txt', 'input_part_1195.txt', 'input_part567.txt', 'input_part_1083.txt', 'input_part402.txt', 'input_part195.txt', 'input_part686.txt', 'input_part591.txt', 'input_part624.txt', 'input_part911.txt', 'input_part815.txt', 'input_part037.txt', 'input_part792.txt', 'input_part_1039.txt', 'input_part_1196.txt', 'input_part536.txt', 'input_part584.txt', 'input_part980.txt', 'input_part_1164.txt', 'input_part360.txt', 'input_part_1211.txt', 'input_part408.txt', 'input_part676.txt', 'input_part512.txt', 'input_part473.txt', 'input_part602.txt', 'input_part_1000.txt', 'input_part058.txt', 'input_part310.txt', 'input_part975.txt', 'input_part_1324.txt', 'input_part103.txt', 'input_part299.txt', 'input_part102.txt', 'input_part078.txt', 'input_part_1190.txt', 'input_part322.txt', 'input_part615.txt', 'input_part553.txt', 'input_part753.txt', 'input_part141.txt', 'input_part246.txt', 'input_part293.txt', 'input_part_1111.txt', 'input_part643.txt', 'input_part_1260.txt', 'input_part_1272.txt', 'input_part_1062.txt', 'input_part691.txt', 'input_part788.txt', 'input_part_1065.txt', 'input_part780.txt', 'input_part282.txt', 'input_part702.txt', 'input_part837.txt', 'input_part933.txt', 'input_part342.txt', 'input_part401.txt', 'input_part531.txt', 'input_part724.txt', 'input_part725.txt', 'input_part016.txt', 'input_part343.txt', 'input_part079.txt', 'input_part_1102.txt', 'input_part_1325.txt', 'input_part851.txt', 'input_part693.txt', 'input_part969.txt', 'input_part961.txt', 'input_part108.txt', 'input_part533.txt', 'input_part_1030.txt', 'input_part230.txt', 'input_part971.txt', 'input_part638.txt', 'input_part746.txt', 'input_part197.txt', 'input_part451.txt', 'input_part_1279.txt', 'input_part566.txt', 'input_part366.txt', 'input_part_1171.txt', 'input_part_1246.txt', 'input_part_1101.txt', 'input_part_1122.txt', 'input_part_1224.txt', 'input_part522.txt', 'input_part_1222.txt', 'input_part989.txt', 'input_part_1187.txt', 'input_part_1303.txt', 'input_part_1296.txt', 'input_part545.txt', 'input_part701.txt', 'input_part468.txt', 'input_part_1286.txt', 'input_part122.txt', 'input_part010.txt', 'input_part208.txt', 'input_part_1218.txt', 'input_part378.txt', 'input_part258.txt', 'input_part665.txt', 'input_part964.txt', 'input_part_1310.txt', 'input_part179.txt', 'input_part211.txt', 'input_part511.txt', 'input_part315.txt', 'input_part803.txt', 'input_part832.txt', 'input_part_1023.txt', 'input_part532.txt', 'input_part627.txt', 'input_part_1004.txt', 'input_part263.txt', 'input_part_1311.txt', 'input_part337.txt', 'input_part424.txt', 'input_part_1186.txt', 'input_part_1010.txt', 'input_part915.txt', 'input_part480.txt', 'input_part098.txt', 'input_part849.txt', 'input_part797.txt', 'input_part678.txt', 'input_part931.txt', 'input_part332.txt', 'input_part_1069.txt', 'input_part446.txt', 'input_part552.txt', 'input_part958.txt', 'input_part505.txt', 'input_part_1074.txt', 'input_part422.txt', 'input_part_1320.txt', 'input_part674.txt', 'input_part739.txt', 'input_part140.txt', 'input_part_1071.txt', 'input_part_1236.txt', 'input_part499.txt', 'input_part595.txt', 'input_part015.txt', 'input_part813.txt', 'input_part829.txt', 'input_part_1242.txt', 'input_part773.txt', 'input_part423.txt', 'input_part518.txt', 'input_part896.txt', 'input_part490.txt', 'input_part369.txt', 'input_part367.txt', 'input_part919.txt', 'input_part_1294.txt', 'input_part356.txt', 'input_part_1123.txt', 'input_part856.txt', 'input_part_1046.txt', 'input_part502.txt', 'input_part_1278.txt', 'input_part838.txt', 'input_part_1259.txt', 'input_part144.txt', 'input_part244.txt', 'input_part_1194.txt', 'input_part991.txt', 'input_part_1018.txt', 'input_part017.txt', 'input_part387.txt', 'input_part761.txt', 'input_part_1308.txt', 'input_part722.txt', 'input_part_1312.txt', 'input_part736.txt', 'input_part_1124.txt', 'input_part_1282.txt', 'input_part542.txt', 'input_part688.txt', 'input_part888.txt', 'input_part_1287.txt', 'input_part115.txt', 'input_part_1206.txt', 'input_part167.txt', 'input_part662.txt', 'input_part781.txt', 'input_part646.txt', 'input_part045.txt', 'input_part129.txt', 'input_part019.txt', 'input_part507.txt', 'input_part850.txt', 'input_part987.txt', 'input_part_1050.txt', 'input_part_1261.txt', 'input_part_1295.txt', 'input_part_1318.txt', 'input_part_1098.txt', 'input_part_1214.txt', 'input_part_1200.txt', 'input_part689.txt', 'input_part719.txt', 'input_part_1095.txt', 'input_part_1132.txt', 'input_part_1040.txt', 'input_part_1192.txt', 'input_part571.txt', 'input_part692.txt', 'input_part471.txt', 'input_part873.txt', 'input_part190.txt', 'input_part734.txt', 'input_part_1127.txt', 'input_part453.txt', 'input_part_1209.txt', 'input_part650.txt', 'input_part500.txt', 'input_part998.txt', 'input_part235.txt', 'input_part304.txt', 'input_part538.txt', 'input_part354.txt', 'input_part212.txt', 'input_part_1277.txt', 'input_part766.txt', 'input_part_1090.txt', 'input_part249.txt', 'input_part_1033.txt', 'input_part841.txt', 'input_part_1025.txt', 'input_part455.txt', 'input_part920.txt', 'input_part882.txt', 'input_part945.txt', 'input_part992.txt', 'input_part194.txt', 'input_part_1291.txt', 'input_part311.txt', 'input_part038.txt', 'input_part463.txt', 'input_part596.txt', 'input_part731.txt', 'input_part_1202.txt', 'input_part_1176.txt', 'input_part_1204.txt', 'input_part700.txt', 'input_part800.txt', 'input_part_1110.txt', 'input_part735.txt', 'input_part707.txt', 'input_part653.txt', 'input_part565.txt', 'input_part864.txt', 'input_part776.txt', 'input_part819.txt', 'input_part_1155.txt', 'input_part_1173.txt', 'input_part708.txt', 'input_part344.txt', 'input_part398.txt', 'input_part593.txt', 'input_part690.txt', 'input_part558.txt', 'input_part_1153.txt', 'input_part916.txt', 'input_part_1100.txt', 'input_part188.txt', 'input_part_1270.txt', 'input_part412.txt', 'input_part926.txt', 'input_part466.txt', 'input_part_1306.txt', 'input_part012.txt', 'input_part827.txt', 'input_part786.txt', 'input_part_1093.txt', 'input_part909.txt', 'input_part914.txt', 'input_part034.txt', 'input_part092.txt', 'input_part744.txt', 'input_part671.txt', 'input_part429.txt', 'input_part084.txt', 'input_part_1112.txt', 'input_part_1234.txt', 'input_part_1269.txt', 'input_part869.txt', 'input_part_1158.txt', 'input_part_1060.txt', 'input_part_1276.txt', 'input_part_1141.txt', 'input_part661.txt', 'input_part005.txt', 'input_part_1300.txt', 'input_part947.txt', 'input_part033.txt', 'input_part_1034.txt', 'input_part_1084.txt', 'input_part_1007.txt', 'input_part795.txt', 'input_part_1117.txt', 'input_part506.txt', 'input_part_1145.txt', 'input_part_1183.txt', 'input_part082.txt', 'input_part810.txt', 'input_part820.txt', 'input_part_1237.txt', 'input_part877.txt', 'input_part523.txt', 'input_part590.txt', 'input_part929.txt', 'input_part_1199.txt', 'input_part_1205.txt', 'input_part738.txt', 'input_part335.txt', 'input_part316.txt', 'input_part716.txt', 'input_part040.txt', 'input_part237.txt', 'input_part755.txt', 'input_part883.txt', 'input_part_1047.txt', 'input_part_1091.txt', 'input_part673.txt', 'input_part_1238.txt', 'input_part525.txt', 'input_part622.txt', 'input_part055.txt', 'input_part456.txt', 'input_part587.txt', 'input_part_1178.txt', 'input_part326.txt', 'input_part_1169.txt', 'input_part706.txt', 'input_part_1159.txt', 'input_part_1193.txt', 'input_part454.txt', 'input_part080.txt', 'input_part611.txt', 'input_part066.txt', 'input_part_1299.txt', 'input_part_1149.txt', 'input_part333.txt', 'input_part569.txt', 'input_part184.txt', 'input_part535.txt', 'input_part677.txt', 'input_part834.txt', 'input_part840.txt', 'input_part907.txt', 'input_part361.txt', 'input_part654.txt', 'input_part939.txt', 'input_part352.txt', 'input_part_1087.txt', 'input_part027.txt', 'input_part881.txt', 'input_part_1009.txt', 'input_part474.txt', 'input_part379.txt', 'input_part794.txt', 'input_part403.txt', 'input_part756.txt', 'input_part858.txt', 'input_part_1172.txt', 'input_part568.txt', 'input_part_1307.txt', 'input_part771.txt', 'input_part521.txt', 'input_part641.txt', 'input_part572.txt', 'input_part_1179.txt', 'input_part941.txt', 'input_part751.txt', 'input_part549.txt', 'input_part874.txt', 'input_part067.txt', 'input_part_1313.txt', 'input_part527.txt', 'input_part737.txt', 'input_part645.txt', 'input_part493.txt', 'input_part_1015.txt', 'input_part_1053.txt', 'input_part_1137.txt', 'input_part695.txt', 'input_part631.txt', 'input_part862.txt', 'input_part283.txt', 'input_part199.txt', 'input_part338.txt', 'input_part242.txt', 'input_part_1064.txt', 'input_part639.txt', 'input_part137.txt', 'input_part944.txt', 'input_part534.txt', 'input_part902.txt', 'input_part_1094.txt', 'input_part_1042.txt', 'input_part_1038.txt', 'input_part386.txt', 'input_part601.txt', 'input_part_1003.txt', 'input_part_1012.txt', 'input_part752.txt', 'input_part220.txt', 'input_part_1086.txt', 'input_part721.txt', 'input_part_1191.txt', 'input_part_1134.txt', 'input_part984.txt', 'input_part059.txt', 'input_part_1168.txt', 'input_part979.txt', 'input_part633.txt', 'input_part679.txt', 'input_part697.txt', 'input_part232.txt', 'input_part830.txt', 'input_part529.txt', 'input_part154.txt', 'input_part713.txt', 'input_part_1150.txt', 'input_part770.txt', 'input_part257.txt', 'input_part349.txt', 'input_part170.txt', 'input_part_1026.txt', 'input_part426.txt', 'input_part383.txt', 'input_part951.txt', 'input_part802.txt', 'input_part_1022.txt', 'input_part_1219.txt', 'input_part685.txt', 'input_part_1252.txt', 'input_part425.txt', 'input_part173.txt', 'input_part_1210.txt', 'input_part_1319.txt', 'input_part334.txt', 'input_part764.txt', 'input_part_1247.txt', 'input_part371.txt', 'input_part_1006.txt', 'input_part218.txt', 'input_part340.txt', 'input_part307.txt', 'input_part373.txt', 'input_part_1085.txt', 'input_part_1230.txt', 'input_part215.txt', 'input_part952.txt', 'input_part_1157.txt', 'input_part_1275.txt', 'input_part913.txt', 'input_part497.txt', 'input_part576.txt', 'input_part921.txt', 'input_part908.txt', 'input_part694.txt', 'input_part254.txt', 'input_part_1131.txt', 'input_part437.txt', 'input_part156.txt', 'input_part629.txt', 'input_part120.txt', 'input_part967.txt', 'input_part363.txt', 'input_part023.txt', 'input_part811.txt', 'input_part399.txt', 'input_part537.txt', 'input_part496.txt', 'input_part052.txt', 'input_part486.txt', 'input_part104.txt', 'input_part054.txt', 'input_part705.txt', 'input_part648.txt', 'input_part223.txt', 'input_part441.txt', 'input_part380.txt', 'input_part477.txt', 'input_part_1167.txt', 'input_part923.txt', 'input_part_1068.txt', 'input_part_1244.txt', 'input_part445.txt', 'input_part886.txt', 'input_part703.txt', 'input_part_1076.txt', 'input_part148.txt', 'input_part392.txt', 'input_part390.txt', 'input_part613.txt', 'input_part_1043.txt', 'input_part_1036.txt', 'input_part285.txt', 'input_part427.txt', 'input_part723.txt', 'input_part754.txt', 'input_part382.txt', 'input_part_1251.txt', 'input_part683.txt', 'input_part443.txt', 'input_part779.txt', 'input_part434.txt', 'input_part504.txt', 'input_part757.txt', 'input_part_1051.txt', 'input_part_1228.txt', 'input_part452.txt', 'input_part_1250.txt', 'input_part_1113.txt', 'input_part867.txt', 'input_part632.txt', 'input_part250.txt', 'input_part516.txt', 'input_part303.txt', 'input_part_1185.txt', 'input_part406.txt', 'input_part_1035.txt', 'input_part550.txt', 'input_part959.txt', 'input_part_1151.txt', 'input_part_1203.txt', 'input_part_1315.txt', 'input_part664.txt', 'input_part_1156.txt', 'input_part_1198.txt', 'input_part663.txt', 'input_part872.txt', 'input_part_1041.txt', 'input_part865.txt', 'input_part_1181.txt', 'input_part265.txt', 'input_part515.txt', 'input_part030.txt', 'input_part091.txt', 'input_part070.txt', 'input_part464.txt', 'input_part_1092.txt', 'input_part948.txt', 'input_part974.txt', 'input_part854.txt', 'input_part589.txt', 'input_part_1088.txt', 'input_part096.txt', 'input_part372.txt', 'input_part901.txt', 'input_part681.txt', 'input_part_1119.txt', 'input_part997.txt', 'input_part699.txt', 'input_part562.txt', 'input_part868.txt', 'input_part_1177.txt', 'input_part_1152.txt', 'input_part018.txt', 'input_part_1249.txt', 'input_part305.txt', 'input_part954.txt', 'input_part375.txt', 'input_part461.txt', 'input_part513.txt', 'input_part575.txt', 'input_part709.txt', 'input_part519.txt', 'input_part843.txt', 'input_part_1301.txt', 'input_part628.txt', 'input_part600.txt', 'input_part200.txt', 'input_part548.txt', 'input_part_1081.txt', 'input_part210.txt', 'input_part431.txt', 'input_part817.txt', 'input_part598.txt', 'input_part978.txt', 'input_part554.txt', 'input_part651.txt', 'input_part_1063.txt', 'input_part_1037.txt', 'input_part355.txt', 'input_part966.txt', 'input_part540.txt', 'input_part240.txt', 'input_part912.txt', 'input_part730.txt', 'input_part133.txt', 'input_part384.txt', 'input_part090.txt', 'input_part801.txt', 'input_part859.txt', 'input_part391.txt', 'input_part_1292.txt', 'input_part124.txt', 'input_part_1170.txt', 'input_part630.txt', 'input_part680.txt', 'input_part742.txt', 'input_part_1297.txt', 'input_part_1097.txt', 'input_part623.txt', 'input_part768.txt', 'input_part364.txt', 'input_part_1075.txt', 'input_part_1184.txt', 'input_part743.txt', 'input_part289.txt', 'input_part809.txt', 'input_part_1245.txt', 'input_part238.txt', 'input_part604.txt', 'input_part_1136.txt', 'input_part_1302.txt', 'input_part870.txt', 'input_part758.txt', 'input_part327.txt', 'input_part389.txt', 'input_part498.txt', 'input_part_1182.txt', 'input_part799.txt', 'input_part357.txt', 'input_part336.txt', 'input_part226.txt', 'input_part778.txt', 'input_part362.txt', 'input_part_1197.txt', 'input_part397.txt', 'input_part_1223.txt', 'input_part469.txt', 'input_part_1285.txt', 'input_part880.txt', 'input_part_1293.txt', 'input_part_1323.txt', 'input_part_1175.txt', 'input_part_1160.txt', 'input_part097.txt', 'input_part835.txt', 'input_part_1126.txt', 'input_part_1243.txt', 'input_part281.txt', 'input_part626.txt', 'input_part581.txt', 'input_part787.txt', 'input_part433.txt', 'input_part415.txt', 'input_part_1240.txt', 'input_part559.txt', 'input_part348.txt', 'input_part_1020.txt', 'input_part268.txt', 'input_part_1079.txt', 'input_part618.txt', 'input_part_1125.txt', 'input_part479.txt', 'input_part_1028.txt', 'input_part_1321.txt', 'input_part798.txt', 'input_part570.txt', 'input_part579.txt', 'input_part852.txt', 'input_part_1225.txt', 'input_part597.txt', 'input_part_1280.txt', 'input_part_1032.txt', 'input_part635.txt', 'input_part712.txt', 'input_part938.txt', 'input_part_1052.txt', 'input_part153.txt', 'input_part821.txt', 'input_part_1283.txt', 'input_part450.txt', 'input_part280.txt', 'input_part875.txt', 'input_part_1103.txt', 'input_part_1268.txt', 'input_part187.txt', 'input_part_1304.txt', 'input_part492.txt', 'input_part619.txt', 'input_part_1057.txt', 'input_part_1258.txt', 'input_part286.txt', 'input_part_1274.txt', 'input_part207.txt', 'input_part009.txt', 'input_part444.txt', 'input_part319.txt', 'input_part750.txt', 'input_part644.txt', 'input_part586.txt', 'input_part472.txt', 'input_part_1016.txt', 'input_part526.txt', 'input_part973.txt', 'input_part_1180.txt', 'input_part476.txt', 'input_part_1298.txt', 'input_part417.txt', 'input_part394.txt', 'input_part271.txt', 'input_part607.txt', 'input_part049.txt', 'input_part621.txt', 'input_part291.txt', 'input_part900.txt', 'input_part749.txt', 'input_part985.txt', 'input_part925.txt', 'input_part_1253.txt', 'input_part605.txt', 'input_part481.txt', 'input_part495.txt', 'input_part609.txt', 'input_part656.txt', 'input_part828.txt', 'input_part899.txt', 'input_part203.txt', 'input_part640.txt', 'input_part796.txt', 'input_part793.txt', 'input_part205.txt', 'input_part_1140.txt', 'input_part582.txt', 'input_part095.txt', 'input_part442.txt', 'input_part763.txt', 'input_part772.txt', 'input_part647.txt', 'input_part667.txt', 'input_part976.txt', 'input_part_1188.txt', 'input_part193.txt', 'input_part_1254.txt', 'input_part_1059.txt', 'input_part726.txt', 'input_part_1019.txt', 'input_part_1133.txt', 'input_part_1154.txt', 'input_part413.txt', 'input_part769.txt', 'input_part669.txt', 'input_part013.txt', 'input_part982.txt', 'input_part642.txt', 'input_part350.txt', 'input_part025.txt', 'input_part727.txt', 'input_part225.txt', 'input_part720.txt', 'input_part988.txt', 'input_part714.txt', 'input_part_1273.txt', 'input_part393.txt', 'input_part056.txt', 'input_part715.txt', 'input_part672.txt', 'input_part790.txt', 'input_part825.txt', 'input_part728.txt', 'input_part_1031.txt', 'input_part_1078.txt', 'input_part231.txt', 'input_part555.txt', 'input_part904.txt', 'input_part_1055.txt', 'input_part981.txt', 'input_part_1021.txt', 'input_part004.txt', 'input_part359.txt', 'input_part530.txt', 'input_part807.txt', 'input_part826.txt', 'input_part420.txt', 'input_part_1166.txt', 'input_part588.txt', 'input_part824.txt', 'input_part321.txt', 'input_part314.txt', 'input_part563.txt', 'input_part_1267.txt', 'input_part955.txt', 'input_part520.txt', 'input_part_1208.txt', 'input_part081.txt', 'input_part564.txt', 'input_part430.txt', 'input_part_1054.txt', 'input_part845.txt', 'input_part487.txt', 'input_part957.txt', 'input_part381.txt', 'input_part_1058.txt', 'input_part_1105.txt', 'input_part428.txt', 'input_part687.txt', 'input_part_1001.txt', 'input_part652.txt', 'input_part831.txt', 'input_part_1070.txt', 'input_part_1289.txt', 'input_part578.txt', 'input_part_1189.txt', 'input_part467.txt', 'input_part860.txt', 'input_part897.txt', 'input_part_1045.txt', 'input_part528.txt', 'input_part814.txt', 'input_part_1008.txt', 'input_part960.txt', 'input_part603.txt', 'input_part924.txt', 'input_part339.txt', 'input_part395.txt', 'input_part747.txt', 'input_part085.txt', 'input_part583.txt', 'input_part_1226.txt', 'input_part_1146.txt', 'input_part625.txt', 'input_part189.txt', 'input_part462.txt', 'input_part273.txt', 'input_part816.txt', 'input_part684.txt', 'input_part599.txt', 'input_part762.txt', 'input_part508.txt', 'input_part351.txt', 'input_part543.txt', 'input_part791.txt', 'input_part330.txt', 'input_part556.txt', 'input_part_1147.txt', 'input_part_1265.txt', 'input_part374.txt', 'input_part657.txt', 'input_part_1138.txt', 'input_part_1288.txt', 'input_part606.txt', 'input_part765.txt', 'input_part_1305.txt', 'input_part_1061.txt', 'input_part234.txt', 'input_part789.txt', 'input_part_1082.txt', 'input_part949.txt', 'input_part217.txt', 'input_part_1005.txt', 'input_part457.txt', 'input_part_1077.txt', 'input_part243.txt', 'input_part158.txt', 'input_part_1262.txt', 'input_part_1017.txt']\n"
     ]
    }
   ],
   "source": [
    "f_train_split = open(\"train_split.txt\")\n",
    "f_test_split = open(\"test_split.txt\")\n",
    "\n",
    "train_split = f_train_split.read().split('\\n')[:-1]\n",
    "test_split = f_test_split.read().split('\\n')[:-1]\n",
    "\n",
    "print(train_split)\n",
    "\n",
    "#get train and test files\n",
    "train_files = []\n",
    "test_files = []\n",
    "train_file_names = []\n",
    "test_file_names = []\n",
    "for i, file_name in enumerate(file_names):\n",
    "    if file_name in train_split:\n",
    "        train_files.append(files[i])\n",
    "        train_file_names.append(file_name)\n",
    "    elif file_name in test_split:\n",
    "        test_files.append(files[i])\n",
    "        test_file_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(881, 209, 881, 209)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_names),len(test_file_names) , len(train_files), len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfc/.myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cas_to_dataset(cas, label2id): \n",
    "    text = cas.sofa_string \n",
    "    entities = [(e.begin, e.end, e.label) for e in cas.select(\"custom.Span\") if e.label and (e.label == \"Claim\" or e.label == \"Non-claim\")] # sort entities by begin \n",
    "    entities = sorted(entities, key=lambda x: x[0]) # initialize tokenization with offsets \n",
    "    \n",
    "    encoding = tokenizer(text, truncation=True, max_length=512, padding=\"max_length\", return_offsets_mapping=True) \n",
    "    labels = [\"O\"] * len(encoding[\"input_ids\"]) \n",
    "    \n",
    "    for start, end, label in entities:\n",
    "        for i, (tok_start, tok_end) in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if tok_start >= end or tok_end <= start:\n",
    "                continue\n",
    "            if tok_start == start:\n",
    "                labels[i] = f\"B-{label}\"\n",
    "            elif tok_start < end:\n",
    "                labels[i] = f\"I-{label}\"\n",
    "                \n",
    "    # convert labels to ids\n",
    "    label_ids = []\n",
    "    for i, l in enumerate(labels):\n",
    "        token = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][i])\n",
    "        if token.startswith(\"##\"):\n",
    "            label_ids.append(-100)  # ignore subword in loss\n",
    "        else:\n",
    "            label_ids.append(label2id[l])\n",
    "                \n",
    "    return [{ \"input_ids\": encoding[\"input_ids\"], \"attention_mask\": encoding[\"attention_mask\"], \"labels\": label_ids }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cas_to_dataset_stride(cas, label2id, doc_id, max_length=512, stride=128):\n",
    "    text = cas.sofa_string\n",
    "    entities = [\n",
    "        (e.begin, e.end, e.label)\n",
    "        for e in cas.select(\"custom.Span\")\n",
    "        if e.label and (e.label == \"Claim\" or e.label == \"Non-claim\")\n",
    "    ]\n",
    "    entities = sorted(entities, key=lambda x: x[0])\n",
    "\n",
    "    # Tokenize with stride\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for chunk_idx in range(len(encoding[\"input_ids\"])):\n",
    "        offsets = encoding[\"offset_mapping\"][chunk_idx]\n",
    "        input_ids = encoding[\"input_ids\"][chunk_idx]\n",
    "        attention_mask = encoding[\"attention_mask\"][chunk_idx]\n",
    "\n",
    "        labels = [\"O\"] * len(input_ids)\n",
    "\n",
    "        # Assign labels for entities\n",
    "        for start, end, label in entities:\n",
    "            for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "                if tok_start >= end or tok_end <= start:\n",
    "                    continue\n",
    "                if tok_start == start:\n",
    "                    labels[i] = f\"B-{label}\"\n",
    "                elif tok_start < end:\n",
    "                    labels[i] = f\"I-{label}\"\n",
    "\n",
    "        # Convert labels to ids\n",
    "        label_ids = []\n",
    "        for i, l in enumerate(labels):\n",
    "            token = tokenizer.convert_ids_to_tokens(int(input_ids[i]))\n",
    "            if token.startswith(\"##\") or input_ids[i] in tokenizer.all_special_ids:\n",
    "                label_ids.append(-100)  # ignore subwords & specials\n",
    "            else:\n",
    "                label_ids.append(label2id[l])\n",
    "\n",
    "        dataset.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label_ids,\n",
    "            \"offset_mapping\": offsets\n",
    "        })\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cas_to_dataset_sent(cas, label2id, doc_id):\n",
    "    text = cas.sofa_string\n",
    "    entities = [(e.begin, e.end, e.label) for e in cas.select(\"custom.Span\") if e.label and (e.label == \"Claim\" or e.label == \"Non-claim\")]\n",
    "    entities = sorted(entities, key=lambda x: x[0])\n",
    "\n",
    "    sentences = text.split('\\n')\n",
    "    dataset = []\n",
    "    \n",
    "    offset = 0\n",
    "    for sent in sentences:\n",
    "        if not sent.strip():  # skip empty lines\n",
    "            offset += len(sent) + 1  # +1 for newline\n",
    "            continue\n",
    "\n",
    "        sent_start = offset\n",
    "        sent_end = offset + len(sent)\n",
    "\n",
    "\n",
    "        ents = []\n",
    "        for (e_start, e_end, label) in entities:\n",
    "            if e_end <= sent_start or e_start >= sent_end:\n",
    "                continue\n",
    "            ents.append((e_start - sent_start, e_end - sent_start, label))\n",
    "\n",
    "\n",
    "        encoding = tokenizer(sent, truncation=True, max_length=512, padding=\"max_length\", return_offsets_mapping=True)\n",
    "        labels = [\"O\"] * len(encoding[\"input_ids\"])\n",
    "\n",
    "        for start, end, label in ents:\n",
    "            for i, (tok_start, tok_end) in enumerate(encoding[\"offset_mapping\"]):\n",
    "                if tok_start >= end or tok_end <= start:\n",
    "                    continue\n",
    "                if tok_start == start:\n",
    "                    labels[i] = f\"B-{label}\"\n",
    "                elif tok_start < end:\n",
    "                    labels[i] = f\"I-{label}\"\n",
    "                    \n",
    "        label_ids = []\n",
    "        for i, l in enumerate(labels):\n",
    "            token = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][i])\n",
    "            if token.startswith(\"##\"):\n",
    "                label_ids.append(-100)  # ignore subword in loss\n",
    "            else:\n",
    "                label_ids.append(label2id[l])\n",
    "                \n",
    "        global_offsets = [\n",
    "            (sent_start + tok_start, sent_start + tok_end)\n",
    "            for (tok_start, tok_end) in encoding[\"offset_mapping\"]\n",
    "        ]\n",
    "\n",
    "        dataset.append({\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"],\n",
    "            \"labels\": label_ids,\n",
    "            \"offset_mapping\": global_offsets,\n",
    "            \"doc_id\": doc_id,\n",
    "        })\n",
    "    \n",
    "        offset = sent_end + 1  # +1 for the newline\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14846 3609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels', 'offset_mapping', 'doc_id'],\n",
       "    num_rows: 14846\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "#label2id = {'O': 0, 'B-Claim': 1, 'I-Claim': 2, 'B-Claim object': 3, 'I-Claim object': 4,\n",
    "# 'B-Claim span': 5, 'I-Claim span': 6, 'B-Claimer': 7, 'I-Claimer': 8,\n",
    "# 'B-Non-claim': 9, 'I-Non-claim': 10, 'B-Time': 11, 'I-Time': 12}\n",
    "\n",
    "label2id = {'O': 0, 'B-Claim': 1, 'I-Claim': 2,\n",
    " 'B-Non-claim': 3, 'I-Non-claim': 4}\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label2id.keys())}\n",
    "\n",
    "def generateDataset(files, file_names):\n",
    "    data_list = []\n",
    "    for file, filename in zip(files, file_names):\n",
    "        data_list = data_list + cas_to_dataset_sent(file, label2id, filename)\n",
    "    return Dataset.from_list(data_list)\n",
    "\n",
    "train_dataset = generateDataset(train_files, train_file_names)\n",
    "test_dataset = generateDataset(test_files, test_file_names)\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label2id),\n",
    "\n",
    "                                                        id2label=id2label, label2id=label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "output_model_name = \"my_model_sent_2\"\n",
    "args = TrainingArguments(\n",
    "    output_model_name,\n",
    "    report_to=\"none\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, #16\n",
    "    per_device_eval_batch_size=16, #16\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,   \n",
    "    metric_for_best_model=\"f1\",      \n",
    "    greater_is_better=True,           \n",
    "    save_total_limit=1\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    print(p)\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    #return results\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13935/3213858002.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13888' max='92800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13888/92800 4:16:39 < 24:18:34, 0.90 it/s, Epoch 14.96/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>0.295361</td>\n",
       "      <td>0.659487</td>\n",
       "      <td>0.407995</td>\n",
       "      <td>0.992790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.016110</td>\n",
       "      <td>0.370853</td>\n",
       "      <td>0.642051</td>\n",
       "      <td>0.470146</td>\n",
       "      <td>0.993468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.019715</td>\n",
       "      <td>0.426737</td>\n",
       "      <td>0.749744</td>\n",
       "      <td>0.543899</td>\n",
       "      <td>0.993124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.025270</td>\n",
       "      <td>0.478842</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.993508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.024953</td>\n",
       "      <td>0.481086</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.534003</td>\n",
       "      <td>0.993429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.034274</td>\n",
       "      <td>0.532999</td>\n",
       "      <td>0.654359</td>\n",
       "      <td>0.587477</td>\n",
       "      <td>0.993567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.034048</td>\n",
       "      <td>0.527304</td>\n",
       "      <td>0.633846</td>\n",
       "      <td>0.575687</td>\n",
       "      <td>0.993360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.035291</td>\n",
       "      <td>0.497724</td>\n",
       "      <td>0.672821</td>\n",
       "      <td>0.572176</td>\n",
       "      <td>0.993188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.031808</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.594452</td>\n",
       "      <td>0.993116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.045828</td>\n",
       "      <td>0.542178</td>\n",
       "      <td>0.725128</td>\n",
       "      <td>0.620448</td>\n",
       "      <td>0.993339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.046086</td>\n",
       "      <td>0.558880</td>\n",
       "      <td>0.593846</td>\n",
       "      <td>0.575833</td>\n",
       "      <td>0.993274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.045409</td>\n",
       "      <td>0.541700</td>\n",
       "      <td>0.699487</td>\n",
       "      <td>0.610564</td>\n",
       "      <td>0.993188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.040762</td>\n",
       "      <td>0.549573</td>\n",
       "      <td>0.659487</td>\n",
       "      <td>0.599534</td>\n",
       "      <td>0.993240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.041483</td>\n",
       "      <td>0.550473</td>\n",
       "      <td>0.715897</td>\n",
       "      <td>0.622381</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe5907514d0>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe7322a93d0>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe70ec8ab10>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe70f271cd0>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe70ee9ebd0>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe70fc1d2d0>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe70d5db3d0>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe733032910>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe70eec2850>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe70d77bdd0>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe70e5d2850>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe70e41ab90>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe5659e8710>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x7fe7101d1a50>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.myenv/lib/python3.11/site-packages/transformers/trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.myenv/lib/python3.11/site-packages/transformers/trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.myenv/lib/python3.11/site-packages/transformers/trainer.py:3797\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3795\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3797\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3799\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.myenv/lib/python3.11/site-packages/accelerate/accelerator.py:2578\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2576\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.myenv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.myenv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"my_model_sent_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model (stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "#load model from \"my_model_/checkpoint-epoch-xx\"\n",
    "model_path = \"my_model_stride\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_with_tolerance(y_true_spans, y_pred_spans, tolerance=5):\n",
    "\n",
    "    stats = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
    "\n",
    "    for doc_true, doc_pred in zip(y_true_spans, y_pred_spans):\n",
    "        matched_gold = set()\n",
    "\n",
    "        # Count true positives and false positives\n",
    "        for p_start, p_end, p_label in doc_pred:\n",
    "            found_match = False\n",
    "            for i, (g_start, g_end, g_label) in enumerate(doc_true):\n",
    "                if i in matched_gold:\n",
    "                    continue\n",
    "                if g_label == p_label and abs(p_start - g_start) <= tolerance and abs(p_end - g_end) <= tolerance:\n",
    "                    stats[p_label][\"tp\"] += 1\n",
    "                    matched_gold.add(i)\n",
    "                    found_match = True\n",
    "                    break\n",
    "            if not found_match:\n",
    "                stats[p_label][\"fp\"] += 1\n",
    "\n",
    "        # Count false negatives\n",
    "        for i, (g_start, g_end, g_label) in enumerate(doc_true):\n",
    "            if i not in matched_gold:\n",
    "                stats[g_label][\"fn\"] += 1\n",
    "\n",
    "    # Compute per-class precision, recall, F1\n",
    "    results = {}\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    for label, s in stats.items():\n",
    "        tp, fp, fn = s[\"tp\"], s[\"fp\"], s[\"fn\"]\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        results[label] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    # Micro-averaged metrics\n",
    "    micro_precision = total_tp / (total_tp + total_fp) if total_tp + total_fp > 0 else 0\n",
    "    micro_recall = total_tp / (total_tp + total_fn) if total_tp + total_fn > 0 else 0\n",
    "    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if micro_precision + micro_recall > 0 else 0\n",
    "    results[\"micro\"] = {\"precision\": micro_precision, \"recall\": micro_recall, \"f1\": micro_f1}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokens_to_spans(pred_labels, offset_mapping):\n",
    "    spans = []\n",
    "    current_label = None\n",
    "    start_char = None\n",
    "    prev_end_char = None\n",
    "\n",
    "    for (tok_start, tok_end), label in zip(offset_mapping, pred_labels):\n",
    "        # Skip special tokens or padding\n",
    "        if tok_start is None or tok_end is None or tok_start == tok_end:\n",
    "            continue\n",
    "\n",
    "        # Subword token: extend current span if inside one\n",
    "        if label == -100:\n",
    "            if current_label is not None:\n",
    "                prev_end_char = tok_end\n",
    "            continue\n",
    "\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_label:\n",
    "                spans.append((start_char, prev_end_char, current_label))\n",
    "            current_label = label[2:]\n",
    "            start_char = tok_start\n",
    "        elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "            prev_end_char = tok_end\n",
    "        else:  # O or mismatch\n",
    "            if current_label:\n",
    "                spans.append((start_char, prev_end_char, current_label))\n",
    "                current_label = None\n",
    "\n",
    "        prev_end_char = tok_end\n",
    "\n",
    "    if current_label:\n",
    "        spans.append((start_char, prev_end_char, current_label))\n",
    "\n",
    "    return spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_stride_predictions(predictions, label_map):\n",
    "\n",
    "    doc_tokens = defaultdict(dict)  # doc_id -> {token_start: (best_conf, pred_label, true_label)}\n",
    "    doc_offsets = defaultdict(list) # doc_id -> list of (start_char, end_char)\n",
    "    for sample in predictions:\n",
    "        doc_id = sample[\"doc_id\"]\n",
    "        offsets = sample[\"offset_mapping\"]\n",
    "        logits_list = sample[\"logits\"]\n",
    "        labels = sample[\"labels\"]\n",
    "\n",
    "        for (start, end), logits, t in zip(offsets, logits_list, labels):\n",
    "            #if t == -100:\n",
    "            #    continue  # skip subword/special tokens\n",
    "\n",
    "            probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "            best_conf, best_idx = torch.max(probs, dim=-1)\n",
    "            pred_label = label_map[int(best_idx)]\n",
    "            #true_label = label_map[int(t)]\n",
    "\n",
    "            if t == -100:\n",
    "                true_label = -100  # keep so subwords can extend span later\n",
    "            else:\n",
    "                true_label = label_map[int(t)]\n",
    "\n",
    "            # if token already seen, keep the most confident prediction\n",
    "            if start in doc_tokens[doc_id]:\n",
    "                #if best_conf > doc_tokens[doc_id][start][0]:\n",
    "                #    doc_tokens[doc_id][start] = (best_conf, pred_label, true_label)\n",
    "                pass\n",
    "            else:\n",
    "                doc_tokens[doc_id][start] = (best_conf, pred_label, true_label)\n",
    "            \n",
    "            # keep track of document-level offsets\n",
    "            if start not in [s for s, e in doc_offsets[doc_id]]:\n",
    "                doc_offsets[doc_id].append((start, end))\n",
    "\n",
    "    y_true_per_doc = []\n",
    "    y_pred_per_doc = []\n",
    "\n",
    "    for doc_id in doc_tokens:\n",
    "        token_starts = sorted(doc_tokens[doc_id].keys())\n",
    "        doc_true_labels = []\n",
    "        doc_pred_labels = []\n",
    "\n",
    "        for start in token_starts:\n",
    "            _, pred_label, true_label = doc_tokens[doc_id][start]\n",
    "            doc_pred_labels.append(pred_label)\n",
    "            doc_true_labels.append(true_label)\n",
    "\n",
    "        y_true_per_doc.append(doc_true_labels)\n",
    "        y_pred_per_doc.append(doc_pred_labels)\n",
    "\n",
    "        # sort offsets for the document\n",
    "        doc_offsets[doc_id] = sorted(doc_offsets[doc_id], key=lambda x: x[0])\n",
    "    return y_true_per_doc, y_pred_per_doc, doc_offsets\n",
    "\n",
    "\n",
    "\n",
    "pred_spans_ner = []\n",
    "gold_spans_ner = []\n",
    "\n",
    "\n",
    "def compute_metrics_stride(eval_preds):\n",
    "    global pred_spans_ner\n",
    "    global gold_spans_ner\n",
    "    logits, labels = eval_preds  # logits shape: (batch_size, seq_len, num_labels)\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(len(logits)):\n",
    "        predictions.append({\n",
    "            \"doc_id\": test_dataset[i][\"doc_id\"],\n",
    "            \"offset_mapping\": test_dataset[i][\"offset_mapping\"],\n",
    "            \"logits\": logits[i],   # store raw logits, not argmax\n",
    "            \"labels\": labels[i]\n",
    "        })\n",
    "\n",
    "    # merge overlapping chunk predictions using most confident token\n",
    "    y_true, y_pred, doc_offsets  = merge_stride_predictions(predictions, model.config.id2label)\n",
    "    all_pred_spans = []\n",
    "    all_gold_spans = []\n",
    "\n",
    "    for i, doc_id in enumerate(doc_offsets.keys()):\n",
    "        pred_spans = tokens_to_spans(y_pred[i], doc_offsets[doc_id])\n",
    "        gold_spans = tokens_to_spans(y_true[i], doc_offsets[doc_id])\n",
    "        all_pred_spans.append(pred_spans)\n",
    "        all_gold_spans.append(gold_spans)\n",
    "    pred_spans_ner = all_pred_spans\n",
    "    gold_spans_ner = all_gold_spans\n",
    "    return evaluate_with_tolerance(all_gold_spans, all_pred_spans, tolerance=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "pred_spans_ner = []\n",
    "gold_spans_ner = []\n",
    "def compute_metrics_sent(eval_preds):\n",
    "    global pred_spans_ner\n",
    "    global gold_spans_ner\n",
    "    logits, labels = eval_preds  # logits shape: (batch_size, seq_len, num_labels)\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(len(logits)):\n",
    "        predictions.append({\n",
    "            \"doc_id\": test_dataset[i][\"doc_id\"],\n",
    "            \"logits\": logits[i],   # store raw logits, not argmax\n",
    "            \"labels\": labels[i],\n",
    "            \"offset_mapping\": test_dataset[i][\"offset_mapping\"],\n",
    "            \n",
    "        })\n",
    "\n",
    "    # get y_true and y_pred without merging\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for sample in predictions:\n",
    "        \n",
    "        logits_list = sample[\"logits\"]\n",
    "        labels = sample[\"labels\"]\n",
    "        doc_id = sample[\"doc_id\"]\n",
    "        doc_true_labels = []\n",
    "        doc_pred_labels = []\n",
    "\n",
    "        for logits, t in zip(logits_list, labels):\n",
    "            if t == -100:\n",
    "                doc_true_labels.append(-100)\n",
    "                doc_pred_labels.append(-100)\n",
    "                continue  # skip subword/special tokens\n",
    "\n",
    "            probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "            best_conf, best_idx = torch.max(probs, dim=-1)\n",
    "            pred_label = model.config.id2label[int(best_idx)]\n",
    "            true_label = model.config.id2label[int(t)]\n",
    "\n",
    "            doc_pred_labels.append(pred_label)\n",
    "            doc_true_labels.append(true_label)\n",
    "\n",
    "        y_true.append(doc_true_labels)\n",
    "        y_pred.append(doc_pred_labels)\n",
    "\n",
    "    all_pred_spans = []\n",
    "    all_gold_spans = []\n",
    "\n",
    "    pred_spans_per_doc = defaultdict(list)\n",
    "    gold_spans_per_doc = defaultdict(list)\n",
    "    \n",
    "    for i, sample in enumerate(predictions):\n",
    "        doc_id = sample[\"doc_id\"]\n",
    "        offsets = sample[\"offset_mapping\"]\n",
    "        pred_spans = tokens_to_spans(y_pred[i], offsets)\n",
    "        gold_spans = tokens_to_spans(y_true[i], offsets)\n",
    "        pred_spans_per_doc[doc_id].extend(pred_spans)\n",
    "        gold_spans_per_doc[doc_id].extend(gold_spans)\n",
    "\n",
    "    # convert to list of lists per document\n",
    "    all_pred_spans = [sorted(pred_spans_per_doc[doc_id]) for doc_id in pred_spans_per_doc.keys()]\n",
    "    all_gold_spans = [sorted(gold_spans_per_doc[doc_id]) for doc_id in gold_spans_per_doc.keys()]\n",
    "\n",
    "    pred_spans_ner = all_pred_spans\n",
    "    gold_spans_ner = all_gold_spans\n",
    "\n",
    "    return evaluate_with_tolerance(all_gold_spans, all_pred_spans, tolerance=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13935/2892214109.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_sent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [226/226 01:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.04148254171013832,\n",
       " 'eval_model_preparation_time': 0.0056,\n",
       " 'eval_Non-claim': {'precision': 0.6334586466165414,\n",
       "  'recall': 0.764172335600907,\n",
       "  'f1': 0.6927029804727647},\n",
       " 'eval_Claim': {'precision': 0.375,\n",
       "  'recall': 0.25806451612903225,\n",
       "  'f1': 0.3057324840764331},\n",
       " 'eval_micro': {'precision': 0.6187943262411347,\n",
       "  'recall': 0.7158974358974359,\n",
       "  'f1': 0.6638135996195911},\n",
       " 'eval_runtime': 165.5313,\n",
       " 'eval_samples_per_second': 21.803,\n",
       " 'eval_steps_per_second': 1.365}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
